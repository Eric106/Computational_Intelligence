{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pandas import DataFrame, read_parquet, read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "from os import listdir\n",
        "import sys\n",
        "\n",
        "def get_random_sample_data(data: DataFrame, test_ratio: float):\n",
        "    ''' permutation: Randomly permute a sequence or return a permuted range (ndarray).\n",
        "                    If x is a multi-dimensional array, it is only shuffled along its first index.'''\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    # return the \u201cx-first\u201d values\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    return data.iloc[test_indices]\n",
        "\n",
        "\n",
        "def log_to_parquet(in_file: str, out_file: str, file_cols: list, parquet_engine: str):\n",
        "    # LOAD DATA FROM A LOG FILE AND SAVE IT ON A PARQUET FILE TO IMPROVE PERFORMANCE AT READING THE DATA\n",
        "    df = read_csv(in_file, sep=\"\\t\", header=None,\n",
        "                  names=file_cols, low_memory=False)\n",
        "    df.to_parquet(out_file, index=False, engine=parquet_engine)\n",
        "    del df\n",
        "\n",
        "\n",
        "def get_files_inFolder(folder: str, fileType: str):\n",
        "    return list(filter(lambda fileName: \n",
        "                            fileName[-len(fileType):] == fileType,\n",
        "                        listdir(folder)))\n",
        "#---------------------------------------------------------------------------------------------\n",
        "def get_date_sig_data(list_datetimes:list):\n",
        "    datetime_format = [\"year\",\"month\",\"day\",\"hour\",\"minute\",\"second\",\"microsecond\"]\n",
        "    dict_sig_data = {value_d:{\"unique\":set()} for value_d in datetime_format}\n",
        "    sig_value_index = None\n",
        "    for date in list_datetimes:\n",
        "        for value_d in datetime_format:\n",
        "            dict_sig_data[value_d][\"unique\"].add(getattr(date,value_d))\n",
        "            if sig_value_index != None:\n",
        "                break\n",
        "            elif len(dict_sig_data[value_d][\"unique\"]) >=4:\n",
        "                sig_value_index = datetime_format.index(value_d)\n",
        "    sig_values = datetime_format[sig_value_index-1:sig_value_index+1]\n",
        "    return tuple(sig_values)\n",
        "\n",
        "def plot(x,y,title,xlabel,ylabel):\n",
        "    plt.bar(x,y)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "def conn_analysis(log_file: str, sample_data: bool):\n",
        "\n",
        "    list_log_files = get_files_inFolder(\"./\",\"log\")\n",
        "    list_parq_files = get_files_inFolder(\"./\",\"parq\")\n",
        "    complete_name_f = log_file.split('.log')[0]+'.parq'\n",
        "    sample_name_f = log_file.split('.log')[0]+'_sample.parq'\n",
        "    P_ENGINE = \"pyarrow\"\n",
        "    SAMPLE_SIZE = 0.10\n",
        "    df = None\n",
        "    if not (complete_name_f in list_parq_files):\n",
        "        if not log_file in list_log_files:\n",
        "            print(\"ERROR fileNotFound: \"+log_file)\n",
        "            return \n",
        "        try:\n",
        "            log_col_names = [\"ts\", \"uid\", \"id_orig_h\", \"id_orig_p\", \"id_resp_h\", \"id_resp_p\", \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
        "                            \"conn_state\", \"local_orig\", \"missed_bytes\", \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"tunnel_parents\"]\n",
        "            log_to_parquet(in_file=log_file, out_file=complete_name_f,\n",
        "                            file_cols=log_col_names, parquet_engine=P_ENGINE)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return\n",
        "    if sample_data and (not sample_name_f in list_parq_files):\n",
        "        complete_df = read_parquet(complete_name_f, engine=P_ENGINE)\n",
        "        sample_df = get_random_sample_data(complete_df, SAMPLE_SIZE)\n",
        "        del complete_df\n",
        "        sample_df.to_parquet(sample_name_f, index=False, engine=P_ENGINE)\n",
        "        del sample_df\n",
        "\n",
        "    important_cols = [\"ts\",\"id_orig_h\", \"id_resp_p\", \"duration\"]\n",
        "    to_read_file = sample_name_f if sample_data else complete_name_f\n",
        "    df = read_parquet(to_read_file, columns=important_cols)\n",
        "\n",
        "    df = df[important_cols]\n",
        "    df[\"ts\"] = list(map(\n",
        "        lambda date: \n",
        "            dt.fromtimestamp(float(date)),\n",
        "        df[\"ts\"].tolist()))\n",
        "    df[\"duration\"] = list(map(\n",
        "        lambda dur: \n",
        "            float(dur) if not \"-\" in dur else 0.0,\n",
        "        df[\"duration\"].tolist()))\n",
        "    print(df.info())\n",
        "\n",
        "    df_not_web_port = df[(df[\"id_resp_p\"] != 80) &\n",
        "                         (df[\"id_resp_p\"] != 8080)]\n",
        "\n",
        "    print('\\n', \"Not http ports: \")\n",
        "    pprint(df_not_web_port)\n",
        "\n",
        "    df_gp_not_web_port = df_not_web_port.groupby(\n",
        "        important_cols[1:3]).size().to_frame().reset_index()\n",
        "    df_gp_not_web_port.rename(columns={0: \"count\"}, inplace=True)\n",
        "    df_gp_not_web_port.sort_values(by=\"count\", ascending=False, inplace=True)\n",
        "    \n",
        "    print('\\n', \"Not http ports count: \")\n",
        "    pprint(df_gp_not_web_port)\n",
        "    \n",
        "    sig_date1, sig_date2 = get_date_sig_data(df[\"ts\"].tolist())\n",
        "    sig_date_col= '-'.join([sig_date1,sig_date2])\n",
        "    df_not_web_port.insert(len(df_not_web_port.columns),sig_date_col,list(map(\n",
        "        lambda date:\n",
        "            str(getattr(date,sig_date1))+\"-\"+str(getattr(date,sig_date2)),\n",
        "        df_not_web_port[\"ts\"].tolist()\n",
        "    )))\n",
        "    df_gp_not_web_port = df_not_web_port.groupby(sig_date_col).size().to_frame().reset_index()\n",
        "    df_gp_not_web_port.rename(columns={0: \"count\"}, inplace=True)\n",
        "    df_gp_not_web_port.sort_values(by=sig_date_col, inplace=True, key=lambda item: int(item.split('-')[0]))\n",
        "    df_gp_not_web_port.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    print('\\n', \"Not http ports groupby \"+sig_date_col+\": \")\n",
        "    pprint(df_gp_not_web_port)\n",
        "\n",
        "    df_long_conn = df[df[\"duration\"] > 5]\n",
        "    df_long_conn = df_long_conn.sort_values(by=\"duration\", ascending=False)\n",
        "\n",
        "    print('\\n', \"Long duration connections: \")\n",
        "    pprint(df_long_conn)\n",
        "\n",
        "\n",
        "def main():\n",
        "    intiTime = time()\n",
        "\n",
        "    # conn_analysis(log_file=\"conn.log\", sample_data=False)\n",
        "    conn_analysis(log_file=\"conn.log\", sample_data=True)\n",
        "\n",
        "    elapsedTime = round(time()-intiTime, 2)\n",
        "    elapsedTime = str(elapsedTime/60) + \\\n",
        "        \"m\" if elapsedTime >= 60 else str(elapsedTime)+\"s\"\n",
        "    print(\"\\nTiempo del proceso --->\", elapsedTime)\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}