{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pandas import DataFrame, read_parquet, read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "from os import listdir\n",
        "import sys\n",
        "\n",
        "def get_random_sample_data(data: DataFrame, test_ratio: float):\n",
        "    ''' permutation: Randomly permute a sequence or return a permuted range (ndarray).\n",
        "                    If x is a multi-dimensional array, it is only shuffled along its first index.'''\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    # return the “x-first” values\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    return data.iloc[test_indices]\n",
        "\n",
        "\n",
        "def log_to_parquet(in_file: str, out_file: str, file_cols: list, parquet_engine: str):\n",
        "    # LOAD DATA FROM A LOG FILE AND SAVE IT ON A PARQUET FILE TO IMPROVE PERFORMANCE AT READING THE DATA\n",
        "    df = read_csv(in_file, sep=\"\\t\", header=None,\n",
        "                  names=file_cols, low_memory=False)\n",
        "    df.to_parquet(out_file, index=False, engine=parquet_engine)\n",
        "    del df\n",
        "\n",
        "\n",
        "def get_files_inFolder(folder: str, fileType: str):\n",
        "    return list(filter(lambda fileName: \n",
        "                            fileName[-len(fileType):] == fileType,\n",
        "                        listdir(folder)))\n",
        "#---------------------------------------------------------------------------------------------\n",
        "def get_date_sig_data(list_datetimes:list):\n",
        "    datetime_format = [\"year\",\"month\",\"day\",\"hour\",\"minute\",\"second\",\"microsecond\"]\n",
        "    dict_sig_data = {value_d:{\"unique\":set()} for value_d in datetime_format}\n",
        "    sig_value_index = None\n",
        "    for date in list_datetimes:\n",
        "        for value_d in datetime_format:\n",
        "            dict_sig_data[value_d][\"unique\"].add(getattr(date,value_d))\n",
        "            if sig_value_index != None:\n",
        "                break\n",
        "            elif len(dict_sig_data[value_d][\"unique\"]) >=4:\n",
        "                sig_value_index = datetime_format.index(value_d)\n",
        "    sig_values = datetime_format[sig_value_index-1:sig_value_index+1]\n",
        "    return tuple(sig_values)\n",
        "\n",
        "def plot(x,y,title,xlabel,ylabel):\n",
        "    plt.bar(x,y)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "def conn_analysis(log_file: str, sample_data: bool):\n",
        "\n",
        "    list_log_files = get_files_inFolder(\"./\",\"log\")\n",
        "    list_parq_files = get_files_inFolder(\"./\",\"parq\")\n",
        "    complete_name_f = log_file.split('.log')[0]+'.parq'\n",
        "    sample_name_f = log_file.split('.log')[0]+'_sample.parq'\n",
        "    P_ENGINE = \"pyarrow\"\n",
        "    SAMPLE_SIZE = 0.10\n",
        "    df = None\n",
        "    if not (complete_name_f in list_parq_files):\n",
        "        if not log_file in list_log_files:\n",
        "            print(\"ERROR fileNotFound: \"+log_file)\n",
        "            return \n",
        "        try:\n",
        "            log_col_names = [\"ts\", \"uid\", \"id_orig_h\", \"id_orig_p\", \"id_resp_h\", \"id_resp_p\", \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
        "                            \"conn_state\", \"local_orig\", \"missed_bytes\", \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"tunnel_parents\"]\n",
        "            log_to_parquet(in_file=log_file, out_file=complete_name_f,\n",
        "                            file_cols=log_col_names, parquet_engine=P_ENGINE)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return\n",
        "    if sample_data and (not sample_name_f in list_parq_files):\n",
        "        complete_df = read_parquet(complete_name_f, engine=P_ENGINE)\n",
        "        sample_df = get_random_sample_data(complete_df, SAMPLE_SIZE)\n",
        "        del complete_df\n",
        "        sample_df.to_parquet(sample_name_f, index=False, engine=P_ENGINE)\n",
        "        del sample_df\n",
        "\n",
        "    important_cols = [\"ts\",\"id_orig_h\", \"id_resp_p\", \"duration\"]\n",
        "    to_read_file = sample_name_f if sample_data else complete_name_f\n",
        "    df = read_parquet(to_read_file, columns=important_cols)\n",
        "\n",
        "    df = df[important_cols]\n",
        "    df[\"ts\"] = list(map(\n",
        "        lambda date: \n",
        "            dt.fromtimestamp(float(date)),\n",
        "        df[\"ts\"].tolist()))\n",
        "    df[\"duration\"] = list(map(\n",
        "        lambda dur: \n",
        "            float(dur) if not \"-\" in dur else 0.0,\n",
        "        df[\"duration\"].tolist()))\n",
        "    print(df.info())\n",
        "\n",
        "    df_not_web_port = df[(df[\"id_resp_p\"] != 80) &\n",
        "                         (df[\"id_resp_p\"] != 8080)]\n",
        "\n",
        "    print('\\n', \"Not http ports: \")\n",
        "    pprint(df_not_web_port)\n",
        "\n",
        "    df_gp_not_web_port = df_not_web_port.groupby(\n",
        "        important_cols[1:3]).size().to_frame().reset_index()\n",
        "    df_gp_not_web_port.rename(columns={0: \"count\"}, inplace=True)\n",
        "    df_gp_not_web_port.sort_values(by=\"count\", ascending=False, inplace=True)\n",
        "    \n",
        "    print('\\n', \"Not http ports count: \")\n",
        "    pprint(df_gp_not_web_port)\n",
        "    \n",
        "    sig_date1, sig_date2 = get_date_sig_data(df[\"ts\"].tolist())\n",
        "    df_not_web_port.insert(len(df_not_web_port.columns),sig_date1,list(map(\n",
        "        lambda date:\n",
        "            getattr(date,sig_date1),\n",
        "        df_not_web_port[\"ts\"].tolist()\n",
        "    )))\n",
        "    df_not_web_port.insert(len(df_not_web_port.columns),sig_date2,list(map(\n",
        "        lambda date:\n",
        "            getattr(date,sig_date2),\n",
        "        df_not_web_port[\"ts\"].tolist()\n",
        "    )))\n",
        "    df_gp_not_web_port = df_not_web_port.groupby([sig_date1, sig_date2]).size().to_frame().reset_index()\n",
        "    df_gp_not_web_port.rename(columns={0: \"count\"}, inplace=True)\n",
        "    df_gp_not_web_port.sort_values(by=[sig_date1, sig_date2], inplace=True)\n",
        "    df_gp_not_web_port.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    print('\\n', \"Not http ports groupby \"+str([sig_date1, sig_date2])+\": \")\n",
        "    pprint(df_gp_not_web_port)\n",
        "\n",
        "    df_long_conn = df[df[\"duration\"] > 5]\n",
        "    df_long_conn = df_long_conn.sort_values(by=\"duration\", ascending=False)\n",
        "\n",
        "    print('\\n', \"Long duration connections: \")\n",
        "    pprint(df_long_conn)\n",
        "\n",
        "\n",
        "def main():\n",
        "    intiTime = time()\n",
        "\n",
        "    # conn_analysis(log_file=\"conn.log\", sample_data=False)\n",
        "    conn_analysis(log_file=\"conn.log\", sample_data=True)\n",
        "\n",
        "    elapsedTime = round(time()-intiTime, 2)\n",
        "    elapsedTime = str(elapsedTime/60) + \\\n",
        "        \"m\" if elapsedTime >= 60 else str(elapsedTime)+\"s\"\n",
        "    print(\"\\nTiempo del proceso --->\", elapsedTime)\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2269435 entries, 0 to 2269434\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Dtype         \n",
            "---  ------     -----         \n",
            " 0   ts         datetime64[ns]\n",
            " 1   id_orig_h  object        \n",
            " 2   id_resp_p  int64         \n",
            " 3   duration   float64       \n",
            "dtypes: datetime64[ns](1), float64(1), int64(1), object(1)\n",
            "memory usage: 69.3+ MB\n",
            "None\n",
            "\n",
            " Not http ports: \n",
            "                             ts        id_orig_h  id_resp_p  duration\n",
            "0       2012-03-16 07:18:01.710  192.168.202.110      39189      0.02\n",
            "1       2012-03-17 09:51:17.370  192.168.202.140         42      0.00\n",
            "2       2012-03-16 11:24:04.360  192.168.202.110      37695      0.00\n",
            "3       2012-03-16 07:04:53.760   192.168.202.83      10626      0.00\n",
            "4       2012-03-16 11:37:53.960  192.168.202.102       1049      0.00\n",
            "...                         ...              ...        ...       ...\n",
            "2269430 2012-03-16 08:25:54.850   192.168.202.83       5862      0.00\n",
            "2269431 2012-03-17 10:58:15.040   192.168.202.83       5925      0.00\n",
            "2269432 2012-03-17 12:13:34.680   192.168.202.83      14238      0.00\n",
            "2269433 2012-03-16 07:29:00.230   192.168.202.83       3737      0.00\n",
            "2269434 2012-03-16 07:50:02.700   192.168.202.83       6346      0.00\n",
            "\n",
            "[2195991 rows x 4 columns]\n",
            "\n",
            " Not http ports count: \n",
            "                                  id_orig_h  id_resp_p  count\n",
            "263987  2001:dbb:c18:202:20c:29ff:fe18:b667        445   4813\n",
            "198567                       192.168.204.45        443   4796\n",
            "9154                        192.168.202.110        443   3938\n",
            "125376                      192.168.202.140        443   3730\n",
            "7730                        192.168.202.108        443   3466\n",
            "...                                     ...        ...    ...\n",
            "186925                       192.168.202.79      60711      1\n",
            "186922                       192.168.202.79      60705      1\n",
            "186918                       192.168.202.79      60699      1\n",
            "186916                       192.168.202.79      60696      1\n",
            "266842            fe80::f2de:f1ff:fe9b:ad6a        134      1\n",
            "\n",
            "[266843 rows x 3 columns]\n",
            "\n",
            " Not http ports groupby ['hour', 'minute']: \n",
            "     hour  minute  count\n",
            "0       6      27    198\n",
            "1       6      28    643\n",
            "2       6      29    618\n",
            "3       6      30    572\n",
            "4       6      31    707\n",
            "..    ...     ...    ...\n",
            "608    16      35    277\n",
            "609    16      36    318\n",
            "610    16      37    291\n",
            "611    16      38    323\n",
            "612    16      39    321\n",
            "\n",
            "[613 rows x 3 columns]\n",
            "\n",
            " Long duration connections: \n",
            "                                ts        id_orig_h  id_resp_p     duration\n",
            "856928  2012-03-17 09:59:01.430000    192.168.1.254       1900  8649.390000\n",
            "1463241 2012-03-17 07:01:07.270000   192.168.202.80         22  7486.550000\n",
            "199253  2012-03-16 13:46:09.730000   192.168.202.76        443  6036.040000\n",
            "308947  2012-03-16 13:46:53.070000   192.168.25.100        443  5981.700000\n",
            "2264321 2012-03-16 13:55:01.990000   192.168.204.45      55554  5504.880000\n",
            "...                            ...              ...        ...          ...\n",
            "984585  2012-03-17 07:04:02.899999   192.168.204.45       9995     5.000001\n",
            "948445  2012-03-17 09:18:15.859999  192.168.202.140      49154     5.000001\n",
            "654352  2012-03-16 14:52:05.299999   192.168.202.87         53     5.000001\n",
            "985200  2012-03-17 08:59:24.499999  192.168.202.140      49153     5.000001\n",
            "1444428 2012-03-17 09:17:52.939999  192.168.202.140      49153     5.000001\n",
            "\n",
            "[8652 rows x 4 columns]\n",
            "\n",
            "Tiempo del proceso ---> 11.03s\n"
          ]
        }
      ],
      "execution_count": 1
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('c_int': conda)",
      "metadata": {
        "interpreter": {
          "hash": "96fa440b9d77aea783129ded58c6f019b551a52ff6ea115cc8177f8e4e67799c"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}